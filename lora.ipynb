{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# const\n",
    "DATASET = 'Flmc/DISC-Med-SFT'\n",
    "BASE_MODEL = 'Qwen/Qwen2.5-0.5B'\n",
    "SFT_MODEL = 'qwen2.5-0.5b-disc-med-sft'\n",
    "MAX_TENSOR_DIM = 68\n",
    "MAX_SIZE = 1000\n",
    "MAX_BATCH_SIZE = 128\n",
    "MIN_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "LEARNING_RATE = 2e-3\n",
    "TRAIN_EPOCHS = 32"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf71207124f2304f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "disc_med_sft = load_dataset(DATASET, cache_dir='./cache')['train']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# peek\n",
    "disc_med_sft"
   ],
   "id": "ca6aba3069d0cc75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputs = list[str]()\n",
    "labels = list[str]()\n",
    "conversation_pair = list[tuple]()\n",
    "conversations_trunks = disc_med_sft['conversation']\n",
    "conversations_trunk = list[dict]()\n",
    "for conversation_trunk in conversations_trunks:\n",
    "    for a_conversation in conversation_trunk:\n",
    "        conversations_trunk.append(a_conversation)\n",
    "conversations = list[dict]()\n",
    "for index, conversation in enumerate(conversations_trunk):\n",
    "    if (\n",
    "            index + 1 < len(conversations_trunk)\n",
    "            and conversation['role'] == 'user' \n",
    "            and conversations_trunk[index + 1]['role'] == 'assistant'\n",
    "    ):\n",
    "        conversations.append((conversation, conversations_trunk[index + 1]))\n",
    "for conversation in conversations[:MAX_SIZE]:\n",
    "    inputs.append(conversation[0]['content'])\n",
    "    labels.append(conversation[1]['content'])\n",
    "print(f'{len(inputs)} inputs, {len(labels)} labels')\n",
    "train_inputs, eval_inputs, train_labels, eval_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(f'{len(train_inputs)} train inputs, {len(train_labels)} train labels')\n",
    "print(f'{len(eval_inputs)} eval inputs, {len(eval_labels)} eval labels')\n",
    "{\n",
    "    'train inputs': train_inputs[:10],\n",
    "    'train labels': train_labels[:10],\n",
    "    'eval inputs': eval_inputs[:10],\n",
    "    'eval labels': eval_labels[:10]\n",
    "}"
   ],
   "id": "306846046f78d632",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find batch size from train and eval set size\n",
    "from sympy import factorint\n",
    "from random import choice\n",
    "\n",
    "ADAPTIVE_TRAIN_BATCH_SIZE_SET = factorint(len(train_inputs), multiple=True, limit=MAX_BATCH_SIZE)\n",
    "ADAPTIVE_TRAIN_BATCH_SIZE = choice(ADAPTIVE_TRAIN_BATCH_SIZE_SET)\n",
    "while not (MIN_BATCH_SIZE <= ADAPTIVE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS <= MAX_BATCH_SIZE):\n",
    "    ADAPTIVE_TRAIN_BATCH_SIZE = choice(ADAPTIVE_TRAIN_BATCH_SIZE_SET)\n",
    "ADAPTIVE_EVAL_BATCH_SIZE_SET = factorint(len(eval_inputs), multiple=True, limit=MAX_BATCH_SIZE)\n",
    "ADAPTIVE_EVAL_BATCH_SIZE = choice(ADAPTIVE_EVAL_BATCH_SIZE_SET)\n",
    "while not (MIN_BATCH_SIZE <= ADAPTIVE_EVAL_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS <= MAX_BATCH_SIZE):\n",
    "    ADAPTIVE_EVAL_BATCH_SIZE = choice(ADAPTIVE_EVAL_BATCH_SIZE_SET)\n",
    "{\n",
    "    'train_batch_size_set': ADAPTIVE_TRAIN_BATCH_SIZE_SET,\n",
    "    'eval_batch_size_set': ADAPTIVE_EVAL_BATCH_SIZE_SET,\n",
    "    'train batch size': ADAPTIVE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS, \n",
    "    'eval batch size': ADAPTIVE_EVAL_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "}"
   ],
   "id": "63686df336fa7d2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, cache_dir='./cache')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir='./cache')\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ],
   "id": "c0c7c3b12d8b7782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prepare train data\n",
    "from datasets import Dataset\n",
    "\n",
    "max_len = MAX_TENSOR_DIM\n",
    "tokenized_train_inputs = tokenizer(\n",
    "    train_inputs, \n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_train_labels = tokenizer(\n",
    "    train_labels, \n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_eval_inputs = tokenizer(\n",
    "    eval_inputs, \n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_eval_labels = tokenizer(\n",
    "    eval_labels, \n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "train_dataset_raw = {\n",
    "    'input_ids': tokenized_train_inputs['input_ids'],\n",
    "    'attention_mask': tokenized_train_inputs['attention_mask'],\n",
    "    'labels': tokenized_train_labels['input_ids']\n",
    "}\n",
    "eval_dataset_raw = {\n",
    "    'input_ids': tokenized_eval_inputs['input_ids'],\n",
    "    'attention_mask': tokenized_eval_inputs['attention_mask'],\n",
    "    'labels': tokenized_eval_labels['input_ids']\n",
    "}\n",
    "train_dataset = Dataset.from_dict(train_dataset_raw)\n",
    "eval_dataset = Dataset.from_dict(eval_dataset_raw)\n",
    "{'train_dataset': train_dataset_raw,'eval_dataset': eval_dataset_raw}"
   ],
   "id": "1e3db1c69e87be63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    use_cpu=True,\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=ADAPTIVE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=ADAPTIVE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    log_level='info',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=16,\n",
    "    logging_strategy='steps',\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=16,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=16,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ").train()"
   ],
   "id": "22098b4e73b87304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save\n",
    "lora_model.save_pretrained(f'./model/{SFT_MODEL}')"
   ],
   "id": "4e6ddf61f42358b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test\n",
    "model = AutoModelForCausalLM.from_pretrained(f'./model/{SFT_MODEL}')\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe('足部骨折。你好大夫，谢谢您百忙中的时间。请问骨折对位可以吗？内侧契骨是稍有错位吗？') # 您好，我很高兴能为您提供帮助，根据您的描述，骨折的对位情况还可以。但是，为了更准确地评估情况，我是否可以看一下术前的片子呢？"
   ],
   "id": "c6ad52310e40790b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
